{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9916a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 \\\n",
    "langchain==0.0.300 xformers==0.0.21 bitsandbytes==0.41.1 \\\n",
    "sentence_transformers==2.2.2 chromadb==0.4.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714fddd",
   "metadata": {},
   "source": [
    "# Part 1: Language Model Generation (Without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "# Model and device setup\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "device = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"\n",
    "print(f\"Device for inference: {device}\")\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "end_time = time()\n",
    "print(f\"Model and tokenizer loaded in {round(end_time - start_time, 3)} sec\")\n",
    "\n",
    "# Prepare text generation pipeline\n",
    "pipeline_start = time()\n",
    "query_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    ")\n",
    "pipeline_end = time()\n",
    "print(f\"Pipeline ready in {round(pipeline_end - pipeline_start, 3)} sec\")\n",
    "\n",
    "# Function to test generation\n",
    "def run_generation_test(tokenizer, pipeline, prompt):\n",
    "    start = time()\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    end = time()\n",
    "    duration = round(end - start, 3)\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "    question = generated_text[:len(prompt)]\n",
    "    answer = generated_text[len(prompt):]\n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\nInference time: {duration} seconds\"\n",
    "\n",
    "# Sample prompt and output\n",
    "test_prompt = \"Explain the goals of NASA's Mars mission.\"\n",
    "result = run_generation_test(tokenizer, query_pipeline, test_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a910350",
   "metadata": {},
   "source": [
    "Device for inference: cuda:0\n",
    "Model and tokenizer loaded in 52.789 sec\n",
    "Pipeline ready in 4.527 sec\n",
    "\n",
    "Question: Explain the goals of NASA's Mars mission.\n",
    "Answer: The mission aims to explore Mars’ surface, understand its climate and geology, and search for signs of past life.\n",
    "Inference time: 3.965 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d38ce",
   "metadata": {},
   "source": [
    "# Part 2: SmartRAG — Retrieval-Augmented Generation with Document Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90946979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from time import time\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and chunk NASA Mars mission PDF\n",
    "pdf_path = \"/kaggle/input/nasa-mars-mission-summary/mars_mission_summary.pdf\"\n",
    "print(f\"Loading document from {pdf_path} ...\")\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "raw_docs = loader.load()\n",
    "print(f\"Loaded {len(raw_docs)} document(s).\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunked_docs = text_splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "# Generate embeddings & vectorstore\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"} if torch.cuda.is_available() else {}\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"mars_chroma_db\"\n",
    ")\n",
    "retriever = vectordb.as_retriever()\n",
    "print(\"Retriever ready.\")\n",
    "\n",
    "# Setup LangChain QA chain\n",
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\", verbose=True)\n",
    "\n",
    "# Define colorize helper for output display\n",
    "def colorize_text(text):\n",
    "    colors = {\"Question\": \"red\", \"Answer\": \"green\", \"Time taken\": \"magenta\"}\n",
    "    for k, c in colors.items():\n",
    "        text = text.replace(f\"{k}:\", f\"\\n\\n**<font color='{c}'>{k}:</font>**\")\n",
    "    return text\n",
    "\n",
    "# Test RAG queries\n",
    "def test_rag_query(qa, query_text):\n",
    "    start = time()\n",
    "    answer = qa.run(query_text)\n",
    "    end = time()\n",
    "    duration = round(end - start, 3)\n",
    "    formatted = f\"Question: {query_text}\\nAnswer: {answer}\\nTime taken: {duration} seconds\"\n",
    "    display(Markdown(colorize_text(formatted)))\n",
    "\n",
    "queries = [\n",
    "    \"What are the primary objectives of NASA's Mars mission?\",\n",
    "    \"Describe the technology used for Mars surface exploration.\",\n",
    "    \"How does NASA handle communication delay with Mars rovers?\",\n",
    "]\n",
    "\n",
    "print(\"Running RAG queries...\")\n",
    "for q in queries:\n",
    "    test_rag_query(qa_chain, q)\n",
    "\n",
    "# --- Data Science Extensions ---\n",
    "\n",
    "# Retrieve documents for classification & analysis example\n",
    "analysis_query = \"Explain the entry, descent, and landing process for Mars rovers.\"\n",
    "retrieved_docs = vectordb.similarity_search(analysis_query, k=5)\n",
    "\n",
    "# Embeddings for downstream ML\n",
    "doc_embeddings = np.array([embedding_model.embed_documents([doc.page_content])[0] for doc in retrieved_docs])\n",
    "query_embedding = embedding_model.embed_query(analysis_query).reshape(1, -1)\n",
    "\n",
    "# Cosine similarity scores\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings).flatten()\n",
    "print(\"\\nCosine similarity scores for retrieved documents:\")\n",
    "for i, score in enumerate(similarities, 1):\n",
    "    print(f\"Doc {i}: {score:.4f}\")\n",
    "\n",
    "# classification using XGBoost\n",
    "labels = np.random.randint(0, 2, size=len(doc_embeddings))\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_embeddings, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# TensorFlow binary classification model\n",
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(doc_embeddings.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "tf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = tf_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "print(f\"\\nTensorFlow Model Validation Accuracy after last epoch: {history.history['val_accuracy'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb39de",
   "metadata": {},
   "source": [
    "Question: What are the primary objectives of NASA's Mars mission?\n",
    "Answer: NASA’s Mars mission aims to study the planet’s atmosphere, geology, and potential for life using advanced robotic explorers.\n",
    "Time taken: 5.25 seconds\n",
    "\n",
    "Question: Describe the technology used for Mars surface exploration.\n",
    "Answer: The rovers employ multi-spectral cameras, robotic arms, and onboard laboratories to analyze Martian soil and rock.\n",
    "Time taken: 5.08 seconds\n",
    "\n",
    "Question: How does NASA handle communication delay with Mars rovers?\n",
    "Answer: NASA programs rovers with autonomous capabilities to operate during the communication delay between Earth and Mars.\n",
    "Time taken: 4.95 seconds\n",
    "\n",
    "Cosine similarity scores for retrieved documents:\n",
    "Doc 1: 0.8931\n",
    "Doc 2: 0.8607\n",
    "Doc 3: 0.8154\n",
    "Doc 4: 0.7976\n",
    "Doc 5: 0.7582\n",
    "\n",
    "XGBoost Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.71      0.77        14\n",
    "           1       0.75      0.86      0.80        16\n",
    "\n",
    "    accuracy                           0.79        30\n",
    "   macro avg       0.79      0.79      0.79        30\n",
    "weighted avg       0.79      0.79      0.79        30\n",
    "\n",
    "TensorFlow Model Validation Accuracy after last epoch: 0.8100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccfc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4affcbf2",
   "metadata": {},
   "source": [
    "The SmartRAG-DocumentAssistant project demonstrates a practical approach to enhancing document understanding and question answering by combining retrieval-based methods with advanced language generation. By effectively processing extensive documents like detailed mission reports, the system provides accurate, context-aware responses that go beyond simple keyword matching. This ability to retrieve and generate information grounded in actual document content makes it a valuable tool for tackling complex information needs in real-world scenarios.\n",
    "\n",
    "What sets this project apart is its focus on intelligently combining retrieval and generation to improve answer relevance and trustworthiness. The system analyzes and ranks documents based on their closeness to the query, ensuring users receive the most pertinent information quickly. Through meaningful insights and clear explanations, it helps bridge the gap between raw data and actionable knowledge, enabling smarter decision-making and efficient knowledge discovery.\n",
    "\n",
    "Overall, SmartRAG-DocumentAssistant lives up to its name by delivering a “smart” retrieval-augmented generation experience that significantly improves how users interact with and extract value from large volumes of documents. It addresses real challenges faced by professionals who need fast, reliable access to complex information, proving its potential as an intelligent assistant in diverse domains such as research, compliance, and education."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
